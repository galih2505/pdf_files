{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "007_Text Preprocessing",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN343rXBrQ4zVDKuGhv4U6v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dhitology/pdf/blob/master/007_Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIgD8kh_McZZ",
        "colab_type": "text"
      },
      "source": [
        "*Muhammad Apriandito Arya Saputra - 2020*\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dDRxpmiAVlHf"
      },
      "source": [
        "# Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2qu88vCfZlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Menginstall package pandas\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTd3m2vTfaSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Memilih data text untuk melalui proses cleaning\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/dhitology/pdf/master/data/text_jurnal.csv')\n",
        "\n",
        "# Melihat 10 Baris Pertama\n",
        "df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPLDwO0Tk50_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Membuat fungsi Text Cleaning\n",
        "import re\n",
        "def  clean_text(df, text_field, new_text_field_name):\n",
        "    df[new_text_field_name] = df[text_field].str.lower()\n",
        "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))  \n",
        "    # remove numbers\n",
        "    df[new_text_field_name] = df[new_text_field_name].apply(lambda elem: re.sub(r\"\\d+\", \"\", elem))\n",
        "    return df\n",
        "\n",
        "data_clean = clean_text(df, 'text', 'text_clean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZpkn1UusAE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Melihat 10 Baris data pertama\n",
        "data_clean.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6DOYKk-k6f_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mengimport corpus stopwords dari library Nltk\n",
        "import nltk.corpus\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhju5RRhk80V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Membersihkan 'stopwords' yang terdapat didalam teks\n",
        "from nltk.corpus import stopwords\n",
        "stop = stopwords.words('english')\n",
        "\n",
        "data_clean['text_clean'] = data_clean['text_clean'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
        "data_clean.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuA2d9oDk_Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mengimport Parameter punkt\n",
        "import nltk \n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEOhKSR1lEzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Melakukan proses tokenization\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "data_clean['text_tokens'] = data_clean['text_clean'].apply(lambda x: word_tokenize(x))\n",
        "data_clean.head(20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jko6laOwlHAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mengimport library yang dibutuhkan untuk proses Stemming\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAlqWPxUlJVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Membuat Fungsi Stemmer\n",
        "def word_stemmer(text):\n",
        "    stem_text = [PorterStemmer().stem(i) for i in text]\n",
        "    return stem_text\n",
        "    \n",
        "data_clean['text_tokens_stem'] = data_clean['text_tokens'].apply(lambda x: word_stemmer(x))\n",
        "data_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-SEoennlL2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mengimport library yang dibutuhkan untuk proses lemmatization\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxEwrRoclNiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Membuat Fungsi Lemmatization\n",
        "def word_lemmatizer(text):\n",
        "    lem_text = [WordNetLemmatizer().lemmatize(i, pos='n') for i in text]\n",
        "    return lem_text\n",
        "    \n",
        "data_clean['text_tokens_lemma'] = data_clean['text_tokens'].apply(lambda x: word_lemmatizer(x))\n",
        "data_clean.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8BE3dzslPXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Menyimpan data sebagai file .csv\n",
        "data_clean.to_csv('text_clean.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}